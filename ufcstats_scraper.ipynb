{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3f3dc362",
      "metadata": {},
      "source": [
        "# UFC Stats Scraper\n",
        "This notebook adapts the original `ufcstats_scraper.py` script into executable cells. Run the cells from top to bottom to build the scraper and optionally execute the crawl."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43cef2f6",
      "metadata": {},
      "source": [
        "## Environment Setup\n",
        "Install the required dependencies in the active environment before running the scraper. You can either create the provided virtual environment (see `README_environment.md`) or run:\n",
        "```bash\n",
        "pip install requests requests-cache beautifulsoup4 tenacity pandas python-dateutil tqdm\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "id": "a192e331",
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "from datetime import date\n",
        "\n",
        "import pandas as pd\n",
        "import requests\n",
        "import requests_cache\n",
        "from bs4 import BeautifulSoup as BS\n",
        "from dateutil import parser as dateparser\n",
        "from tenacity import retry, stop_after_attempt, wait_exponential\n",
        "from tqdm import tqdm\n",
        "\n",
        "BASE = \"http://ufcstats.com\"\n",
        "DATA_DIR = \"./ufc_out\"\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "\n",
        "EVENTS_CSV = os.path.join(DATA_DIR, \"events.csv\")\n",
        "FIGHTS_CSV = os.path.join(DATA_DIR, \"fights.csv\")\n",
        "FIGHTERS_CSV = os.path.join(DATA_DIR, \"fighters.csv\")\n",
        "TOT_OVERALL_CSV = os.path.join(DATA_DIR, \"fight_totals_overall.csv\")\n",
        "TOT_ROUND_CSV = os.path.join(DATA_DIR, \"fight_totals_round.csv\")\n",
        "SIG_OVERALL_CSV = os.path.join(DATA_DIR, \"fight_sig_overall.csv\")\n",
        "SIG_ROUND_CSV = os.path.join(DATA_DIR, \"fight_sig_round.csv\")\n",
        "FAIL_CSV = os.path.join(DATA_DIR, \"failures.csv\")\n",
        "STATE_JSON = os.path.join(DATA_DIR, \"state.json\")\n",
        "\n",
        "requests_cache.install_cache(os.path.join(DATA_DIR, \"http_cache\"), backend=\"sqlite\", expire_after=60 * 60 * 24 * 7)\n",
        "HEADERS = {\n",
        "    \"User-Agent\": \"UFC research (mailto:you@example.com)\",\n",
        "    \"Accept-Language\": \"en-US,en;q=0.9\",\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "id": "54f7fceb",
      "metadata": {},
      "outputs": [],
      "source": [
        "def normalize_url(u: str) -> str:\n",
        "    if not u:\n",
        "        return u\n",
        "    return u.strip()\n",
        "\n",
        "\n",
        "@retry(stop=stop_after_attempt(5), wait=wait_exponential(multiplier=1, min=2, max=30), reraise=True)\n",
        "def _get(url: str) -> requests.Response:\n",
        "    r = requests.get(normalize_url(url), headers=HEADERS, timeout=30)\n",
        "    r.raise_for_status()\n",
        "    return r\n",
        "\n",
        "\n",
        "def soup(url: str) -> BS:\n",
        "    html = _get(url).text\n",
        "    return BS(html, \"html.parser\")\n",
        "\n",
        "\n",
        "def clean(x: str) -> str:\n",
        "    return re.sub(r\"\\s+\", \" \", x or \"\").strip()\n",
        "\n",
        "\n",
        "def parse_height_to_inches(text):\n",
        "    m = re.search(r\"(\\d+)\\s*'\\s*(\\d+)\", text or \"\")\n",
        "    return int(m.group(1)) * 12 + int(m.group(2)) if m else None\n",
        "\n",
        "\n",
        "def parse_reach_to_inches(text):\n",
        "    m = re.search(r\"(\\d+)\\s*\\\"\", text or \"\")\n",
        "    return int(m.group(1)) if m else None\n",
        "\n",
        "\n",
        "def split_of(text):\n",
        "    m = re.search(r\"(\\d+)\\s*of\\s*(\\d+)\", text or \"\")\n",
        "    return (int(m.group(1)), int(m.group(2))) if m else (None, None)\n",
        "\n",
        "\n",
        "def parse_pct(text):\n",
        "    m = re.search(r\"(\\d+(?:\\.\\d+)?)\\s*%\", text or \"\")\n",
        "    return float(m.group(1)) if m else None\n",
        "\n",
        "\n",
        "def parse_mmss(text):\n",
        "    m = re.search(r\"(\\d+):(\\d+)\", text or \"\")\n",
        "    return int(m.group(1)) * 60 + int(m.group(2)) if m else (0 if text and text.strip() == \"0:00\" else None)\n",
        "\n",
        "\n",
        "def years_between(dob_iso, event_iso):\n",
        "    if not dob_iso or not event_iso:\n",
        "        return None\n",
        "    try:\n",
        "        dob = date.fromisoformat(dob_iso)\n",
        "        evd = date.fromisoformat(event_iso)\n",
        "        return evd.year - dob.year - ((evd.month, evd.day) < (dob.month, dob.day))\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "\n",
        "def load_state():\n",
        "    if os.path.exists(STATE_JSON):\n",
        "        with open(STATE_JSON, \"r\") as f:\n",
        "            return json.load(f)\n",
        "    return {\"event_idx\": 0}\n",
        "\n",
        "\n",
        "def save_state(state):\n",
        "    with open(STATE_JSON, \"w\") as f:\n",
        "        json.dump(state, f)\n",
        "\n",
        "\n",
        "def append_df(path, df):\n",
        "    if df is None or not len(df):\n",
        "        return\n",
        "    header = not os.path.exists(path)\n",
        "    df.to_csv(path, mode=\"a\", header=header, index=False)\n",
        "\n",
        "\n",
        "def existing_ids(path, col):\n",
        "    if not os.path.exists(path):\n",
        "        return set()\n",
        "    try:\n",
        "        return set(pd.read_csv(path, usecols=[col])[col].dropna().astype(str).tolist())\n",
        "    except Exception:\n",
        "        return set()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "id": "2b493232",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "TOTALS_COLS = {\n",
        "    \"KD\": \"kd\",\n",
        "    \"SIG. STR.\": \"sig_str\",\n",
        "    \"SIG. STR. %\": \"sig_str_pct\",\n",
        "    \"TOTAL STR.\": \"total_str\",\n",
        "    \"TD\": \"td\",\n",
        "    \"TD %\": \"td_pct\",\n",
        "    \"SUB. ATT\": \"sub_att\",\n",
        "    \"REV.\": \"rev\",\n",
        "    \"CTRL\": \"ctrl\",\n",
        "}\n",
        "\n",
        "SIG_COLS = {\n",
        "    \"SIG. STR.\": \"sig_str\",\n",
        "    \"SIG. STR. %\": \"sig_str_pct\",\n",
        "    \"HEAD\": \"head\",\n",
        "    \"BODY\": \"body\",\n",
        "    \"LEG\": \"leg\",\n",
        "    \"DISTANCE\": \"distance\",\n",
        "    \"CLINCH\": \"clinch\",\n",
        "    \"GROUND\": \"ground\",\n",
        "}\n",
        "\n",
        "\n",
        "def _extend_colmap(colmap):\n",
        "    augmented = {}\n",
        "    for key, value in colmap.items():\n",
        "        variants = {\n",
        "            key,\n",
        "            key.rstrip('.'),\n",
        "            key.replace('.', ''),\n",
        "            key.replace('.', '').replace('%', '%'),\n",
        "        }\n",
        "        for variant in variants:\n",
        "            normalized = variant.strip()\n",
        "            if not normalized:\n",
        "                continue\n",
        "            if normalized not in colmap and normalized not in augmented:\n",
        "                augmented[normalized] = value\n",
        "    colmap.update(augmented)\n",
        "\n",
        "_extend_colmap(TOTALS_COLS)\n",
        "_extend_colmap(SIG_COLS)\n",
        "\n",
        "\n",
        "def _resolve_col_key(header, colmap):\n",
        "    candidates = [header]\n",
        "    candidates.append(header.rstrip('.'))\n",
        "    candidates.append(header.replace('.', ''))\n",
        "    candidates.append(header.replace('.', '').replace('%', '%'))\n",
        "    for cand in candidates:\n",
        "        normalized = cand.strip()\n",
        "        if not normalized:\n",
        "            continue\n",
        "        if normalized in colmap:\n",
        "            return colmap[normalized]\n",
        "    return None\n",
        "\n",
        "\n",
        "def _normalize_headers(ths):\n",
        "    return [re.sub(r\"\\s+\", \" \", th.get_text(strip=True)).upper() for th in ths]\n",
        "\n",
        "\n",
        "def _split_cell_texts(td):\n",
        "    texts = [clean(p.get_text(\" \", strip=True)) for p in td.select(\".b-fight-details__table-text\")]\n",
        "    if not texts:\n",
        "        text = clean(td.get_text(\" \", strip=True))\n",
        "        texts = [text] if text else []\n",
        "    return [t for t in texts if t is not None]\n",
        "\n",
        "\n",
        "def _rows_from_tr(tds):\n",
        "    columns = [_split_cell_texts(td) for td in tds]\n",
        "    lengths = [len(col) for col in columns if col]\n",
        "    if not lengths:\n",
        "        return []\n",
        "    fighters = max(lengths)\n",
        "    rows = []\n",
        "    for idx in range(fighters):\n",
        "        row = []\n",
        "        for col in columns:\n",
        "            row.append(col[idx] if idx < len(col) else \"\")\n",
        "        rows.append(row)\n",
        "    return rows\n",
        "\n",
        "\n",
        "def _parse_row_values(headers, values, colmap):\n",
        "    vals = {}\n",
        "    for h, text in zip(headers, values):\n",
        "        key = _resolve_col_key(h, colmap)\n",
        "        if not key:\n",
        "            continue\n",
        "        t = text or \"\"\n",
        "        if key in {\"sig_str\", \"total_str\", \"td\", \"head\", \"body\", \"leg\", \"distance\", \"clinch\", \"ground\"}:\n",
        "            landed, att = split_of(t)\n",
        "            vals[f\"{key}_landed\"] = landed\n",
        "            vals[f\"{key}_attempted\"] = att\n",
        "        elif key in {\"sig_str_pct\", \"td_pct\"}:\n",
        "            vals[key] = parse_pct(t)\n",
        "        elif key == \"ctrl\":\n",
        "            vals[\"ctrl_seconds\"] = parse_mmss(t)\n",
        "        elif key in {\"kd\", \"sub_att\", \"rev\"}:\n",
        "            try:\n",
        "                vals[key] = int(t) if t not in {\"--\", \"\"} else None\n",
        "            except Exception:\n",
        "                vals[key] = None\n",
        "    return vals\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def parse_table_block(table, title_text, fight_id, event_id, event_date_iso, red_id, blue_id):\n",
        "    header_section = table.find(\"thead\")\n",
        "    if not header_section:\n",
        "        return [], [], None\n",
        "    header_tr = header_section.find(\"tr\")\n",
        "    if not header_tr:\n",
        "        return [], [], None\n",
        "    headers = _normalize_headers(header_tr.find_all(\"th\", recursive=False))\n",
        "\n",
        "    if any(h in {\"HEAD\", \"BODY\", \"LEG\", \"DISTANCE\", \"CLINCH\", \"GROUND\"} for h in headers):\n",
        "        colmap = SIG_COLS\n",
        "        table_tag = \"significant\"\n",
        "    else:\n",
        "        colmap = TOTALS_COLS\n",
        "        table_tag = \"totals\"\n",
        "\n",
        "    table_classes = table.get(\"class\", []) or []\n",
        "    is_per_round_table = \"js-fight-table\" in table_classes or any(\n",
        "        (thead.get(\"class\") and \"round\" in \" \".join(thead.get(\"class\")))\n",
        "        for thead in table.find_all(\"thead\")\n",
        "    )\n",
        "\n",
        "    overall_rows, per_round_rows = [], []\n",
        "    fighters = [red_id, blue_id]\n",
        "    current_round = None\n",
        "    round_counter = 0\n",
        "\n",
        "    children = [child for child in table.children if getattr(child, \"name\", None) in {\"thead\", \"tbody\"}]\n",
        "    for child in children:\n",
        "        if child.name == \"thead\":\n",
        "            text = clean(child.get_text(\" \", strip=True))\n",
        "            match = re.search(r\"ROUND\\s*(\\d+)\", text, re.IGNORECASE)\n",
        "            if match:\n",
        "                try:\n",
        "                    current_round = int(match.group(1))\n",
        "                    round_counter = current_round\n",
        "                except Exception:\n",
        "                    current_round = None\n",
        "            continue\n",
        "\n",
        "        if child.name != \"tbody\":\n",
        "            continue\n",
        "\n",
        "        for tr in child.find_all(\"tr\", recursive=False):\n",
        "            tds = tr.find_all(\"td\", recursive=False)\n",
        "            if not tds:\n",
        "                continue\n",
        "            row_values = _rows_from_tr(tds)\n",
        "            if not row_values:\n",
        "                continue\n",
        "\n",
        "            assigned_round = current_round\n",
        "            if assigned_round is None and is_per_round_table:\n",
        "                round_counter += 1\n",
        "                assigned_round = round_counter\n",
        "\n",
        "            level = \"overall\" if assigned_round is None else \"round\"\n",
        "            base = {\n",
        "                \"fight_id\": fight_id,\n",
        "                \"event_id\": event_id,\n",
        "                \"event_date\": event_date_iso,\n",
        "                \"level\": level,\n",
        "                \"round\": assigned_round,\n",
        "                \"table\": table_tag,\n",
        "            }\n",
        "\n",
        "            for idx, values in enumerate(row_values):\n",
        "                if idx >= len(fighters):\n",
        "                    break\n",
        "                stats = _parse_row_values(headers, values, colmap)\n",
        "                if not stats:\n",
        "                    continue\n",
        "                row = {**base, \"fighter_id\": fighters[idx]}\n",
        "                row.update(stats)\n",
        "                if level == \"overall\":\n",
        "                    overall_rows.append(row)\n",
        "                else:\n",
        "                    per_round_rows.append(row)\n",
        "\n",
        "        if current_round is not None:\n",
        "            # ensure we only reuse an explicit round until the next header updates it\n",
        "            current_round = None\n",
        "\n",
        "    return overall_rows, per_round_rows, table_tag\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "id": "a752ab63",
      "metadata": {},
      "outputs": [],
      "source": [
        "def list_completed_event_urls():\n",
        "    url = f\"{BASE}/statistics/events/completed?page=all\"\n",
        "    sp = soup(url)\n",
        "    out = []\n",
        "    for a in sp.select('tr.b-statistics__table-row a[href*=\"event-details\"]'):\n",
        "        href = a.get(\"href\")\n",
        "        if href and \"event-details\" in href:\n",
        "            out.append(normalize_url(href))\n",
        "    return sorted(set(out))\n",
        "\n",
        "\n",
        "def parse_event(event_url):\n",
        "    sp = soup(event_url)\n",
        "    event_id = event_url.rsplit(\"/\", 1)[-1]\n",
        "    title = sp.select_one(\"h2.b-content__title\")\n",
        "    name = clean(title.text if title else \"\")\n",
        "    info_items = [clean(li.text) for li in sp.select(\"li.b-list__box-list-item\")]\n",
        "    date_txt = next((i.split(\":\", 1)[1].strip() for i in info_items if i.lower().startswith(\"date:\")), None)\n",
        "    location = next((i.split(\":\", 1)[1].strip() for i in info_items if i.lower().startswith(\"location:\")), None)\n",
        "    try:\n",
        "        date_iso = dateparser.parse(date_txt).date().isoformat() if date_txt else None\n",
        "    except Exception:\n",
        "        date_iso = None\n",
        "    fight_urls = sorted(\n",
        "        set(\n",
        "            normalize_url(a.get(\"href\"))\n",
        "            for a in sp.select('a[href*=\"/fight-details/\"]')\n",
        "            if a.get(\"href\")\n",
        "        )\n",
        "    )\n",
        "    row = {\n",
        "        \"event_id\": event_id,\n",
        "        \"event_url\": event_url,\n",
        "        \"name\": name,\n",
        "        \"date\": date_iso,\n",
        "        \"raw_date\": date_txt,\n",
        "        \"location\": location,\n",
        "    }\n",
        "    return row, fight_urls\n",
        "\n",
        "\n",
        "def parse_fighter(fighter_url):\n",
        "    sp = soup(fighter_url)\n",
        "    fighter_id = fighter_url.rsplit(\"/\", 1)[-1]\n",
        "    name_el = sp.select_one(\"span.b-content__title-highlight\")\n",
        "    name = clean(name_el.text if name_el else \"\")\n",
        "    bio_items = [clean(li.text) for li in sp.select(\"li.b-list__box-list-item\")]\n",
        "    h_in = r_in = stance = dob_iso = None\n",
        "    for it in bio_items:\n",
        "        upper = it.upper()\n",
        "        if upper.startswith(\"HEIGHT:\"):\n",
        "            h_in = parse_height_to_inches(it.split(\":\", 1)[1])\n",
        "        elif upper.startswith(\"REACH:\"):\n",
        "            r_in = parse_reach_to_inches(it.split(\":\", 1)[1])\n",
        "        elif \"STANCE\" in upper:\n",
        "            stance = clean(it.split(\":\", 1)[1])\n",
        "        elif upper.startswith(\"DOB:\"):\n",
        "            try:\n",
        "                dob_iso = dateparser.parse(it.split(\":\", 1)[1]).date().isoformat()\n",
        "            except Exception:\n",
        "                pass\n",
        "    return {\n",
        "        \"fighter_id\": fighter_id,\n",
        "        \"name\": name,\n",
        "        \"height_in\": h_in,\n",
        "        \"reach_in\": r_in,\n",
        "        \"stance\": stance,\n",
        "        \"dob\": dob_iso,\n",
        "    }\n",
        "\n",
        "\n",
        "def parse_fight(fight_url, event_id=None, event_date_iso=None):\n",
        "    sp = soup(fight_url)\n",
        "    fight_id = fight_url.rsplit(\"/\", 1)[-1]\n",
        "\n",
        "    persons = sp.select(\"div.b-fight-details__person\")\n",
        "\n",
        "    def side(div):\n",
        "        a = div.select_one(\"a.b-link.b-fight-details__person-link\")\n",
        "        name = clean(a.text if a else \"\")\n",
        "        link = normalize_url(a.get(\"href\")) if a else None\n",
        "        fid = link.rsplit(\"/\", 1)[-1] if link else None\n",
        "        status_el = div.select_one(\"i.b-fight-details__person-status\")\n",
        "        status = clean(status_el.text if status_el else \"\")\n",
        "        return {\"fighter_id\": fid, \"fighter_url\": link, \"name\": name, \"status\": status}\n",
        "\n",
        "    sides = [side(p) for p in persons[:2]]\n",
        "    red = sides[0] if len(sides) >= 1 else None\n",
        "    blue = sides[1] if len(sides) >= 2 else None\n",
        "    red_id = red[\"fighter_id\"] if red else None\n",
        "    blue_id = blue[\"fighter_id\"] if blue else None\n",
        "\n",
        "\n",
        "    meta_lookup = {}\n",
        "    for block in sp.select('[class*=\"b-fight-details__text-item\"]'):\n",
        "        label_el = block.select_one('.b-fight-details__label')\n",
        "        if not label_el:\n",
        "            continue\n",
        "        label_text = clean(label_el.get_text(\" \", strip=True)).rstrip(':').lower()\n",
        "        if not label_text:\n",
        "            continue\n",
        "        label_el.extract()\n",
        "        value_text = clean(block.get_text(\" \", strip=True))\n",
        "        if not value_text:\n",
        "            continue\n",
        "        meta_lookup[label_text] = value_text\n",
        "\n",
        "    for selector in (\n",
        "        \"p.b-fight-details__text-item\",\n",
        "        \"i.b-fight-details__text-item\",\n",
        "        \"span.b-fight-details__text-item\",\n",
        "    ):\n",
        "        for node in sp.select(selector):\n",
        "            text = clean(node.get_text(\" \", strip=True))\n",
        "            if not text or ':' not in text:\n",
        "                continue\n",
        "            label, value = text.split(':', 1)\n",
        "            label_key = label.strip().lower()\n",
        "            if label_key in meta_lookup:\n",
        "                continue\n",
        "            meta_lookup[label_key] = value.strip()\n",
        "\n",
        "    method = meta_lookup.get('method')\n",
        "    referee = meta_lookup.get('referee')\n",
        "\n",
        "    end_round = None\n",
        "    round_text = meta_lookup.get('round')\n",
        "    if round_text:\n",
        "        m_round = re.search(r\"\\d+\", round_text)\n",
        "        if m_round:\n",
        "            try:\n",
        "                end_round = int(m_round.group(0))\n",
        "            except Exception:\n",
        "                end_round = None\n",
        "\n",
        "    end_time = meta_lookup.get('time')\n",
        "\n",
        "    totals_overall, totals_rounds, sig_overall, sig_rounds = [], [], [], []\n",
        "    seen_tables = set()\n",
        "    for tbl in sp.find_all(\"table\"):\n",
        "        if id(tbl) in seen_tables:\n",
        "            continue\n",
        "        seen_tables.add(id(tbl))\n",
        "        title_el = tbl.find_previous(\"h2\")\n",
        "        title_text = title_el.get_text(\" \", strip=True) if title_el else \"\"\n",
        "        ov, pr, table_tag = parse_table_block(\n",
        "            tbl,\n",
        "            title_text,\n",
        "            fight_id,\n",
        "            event_id,\n",
        "            event_date_iso,\n",
        "            red_id,\n",
        "            blue_id,\n",
        "        )\n",
        "        if not ov and not pr:\n",
        "            continue\n",
        "\n",
        "        if table_tag == \"totals\":\n",
        "            totals_overall.extend(ov)\n",
        "            totals_rounds.extend(pr)\n",
        "        elif table_tag == \"significant\":\n",
        "            sig_overall.extend(ov)\n",
        "            sig_rounds.extend(pr)\n",
        "        elif \"TOTAL\" in (title_text or \"\").upper():\n",
        "            totals_overall.extend(ov)\n",
        "            totals_rounds.extend(pr)\n",
        "        elif \"SIGNIFICANT\" in (title_text or \"\").upper():\n",
        "            sig_overall.extend(ov)\n",
        "            sig_rounds.extend(pr)\n",
        "\n",
        "    fight_row = {\n",
        "        \"fight_id\": fight_id,\n",
        "        \"fight_url\": fight_url,\n",
        "        \"event_id\": event_id,\n",
        "        \"event_date\": event_date_iso,\n",
        "        \"red_id\": red_id,\n",
        "        \"red_name\": red[\"name\"] if red else None,\n",
        "        \"red_result\": red[\"status\"] if red else None,\n",
        "        \"blue_id\": blue_id,\n",
        "        \"blue_name\": blue[\"name\"] if blue else None,\n",
        "        \"blue_result\": blue[\"status\"] if blue else None,\n",
        "        \"method\": method,\n",
        "        \"referee\": referee,\n",
        "        \"end_round\": end_round,\n",
        "        \"end_time\": end_time,\n",
        "    }\n",
        "    return fight_row, sides, totals_overall, totals_rounds, sig_overall, sig_rounds\n",
        "\n",
        "\n",
        "def crawl():\n",
        "    state = load_state()\n",
        "    event_urls = list_completed_event_urls()\n",
        "\n",
        "    done_events = existing_ids(EVENTS_CSV, \"event_id\")\n",
        "    done_fights = existing_ids(FIGHTS_CSV, \"fight_id\")\n",
        "    have_fighter = existing_ids(FIGHTERS_CSV, \"fighter_id\")\n",
        "\n",
        "    print(f\"Found {len(event_urls)} events. Resuming from event index {state['event_idx']}.\")\n",
        "\n",
        "    for ei in tqdm(range(state[\"event_idx\"], len(event_urls)), desc=\"Events\"):\n",
        "        print(f\"Processing event {ei} of {len(event_urls)}\")\n",
        "        eurl = event_urls[ei]\n",
        "        try:\n",
        "            ev_row, fight_urls = parse_event(eurl)\n",
        "        except Exception as exc:\n",
        "            append_df(\n",
        "                FAIL_CSV,\n",
        "                pd.DataFrame([\n",
        "                    {\"url\": eurl, \"type\": \"event\", \"error\": str(exc)}\n",
        "                ]),\n",
        "            )\n",
        "            state[\"event_idx\"] = ei + 1\n",
        "            save_state(state)\n",
        "            continue\n",
        "\n",
        "        if ev_row[\"event_id\"] not in done_events:\n",
        "            append_df(EVENTS_CSV, pd.DataFrame([ev_row]))\n",
        "            done_events.add(ev_row[\"event_id\"])\n",
        "\n",
        "        fights_batch = []\n",
        "        fighters_batch = []\n",
        "        failures_batch = []\n",
        "        tot_overall_batch, tot_round_batch = [], []\n",
        "        sig_overall_batch, sig_round_batch = [], []\n",
        "\n",
        "        for furl in tqdm(fight_urls, leave=False, desc=f\"Fights@{ev_row['event_id']}\"):\n",
        "            fid = furl.rsplit(\"/\", 1)[-1]\n",
        "            if fid in done_fights:\n",
        "                continue\n",
        "            try:\n",
        "                fight_row, sides, t_overall, t_rounds, s_overall, s_rounds = parse_fight(\n",
        "                    furl,\n",
        "                    event_id=ev_row[\"event_id\"],\n",
        "                    event_date_iso=ev_row[\"date\"],\n",
        "                )\n",
        "                fights_batch.append(fight_row)\n",
        "                tot_overall_batch.extend(t_overall)\n",
        "                tot_round_batch.extend(t_rounds)\n",
        "                sig_overall_batch.extend(s_overall)\n",
        "                sig_round_batch.extend(s_rounds)\n",
        "\n",
        "                for s in sides:\n",
        "                    if not s or not s.get(\"fighter_id\") or s[\"fighter_id\"] in have_fighter:\n",
        "                        continue\n",
        "                    try:\n",
        "                        bio = parse_fighter(s[\"fighter_url\"])\n",
        "                        bio[\"age_on_event\"] = years_between(bio.get(\"dob\"), ev_row[\"date\"])\n",
        "                        fighters_batch.append(bio)\n",
        "                        have_fighter.add(bio[\"fighter_id\"])\n",
        "                        time.sleep(0.1)\n",
        "                    except Exception as fighter_exc:\n",
        "                        failures_batch.append(\n",
        "                            {\n",
        "                                \"url\": s[\"fighter_url\"],\n",
        "                                \"type\": \"fighter\",\n",
        "                                \"error\": str(fighter_exc),\n",
        "                            }\n",
        "                        )\n",
        "\n",
        "                done_fights.add(fid)\n",
        "\n",
        "                if len(fights_batch) >= 20:\n",
        "                    append_df(FIGHTS_CSV, pd.DataFrame(fights_batch))\n",
        "                    fights_batch.clear()\n",
        "                if len(fighters_batch) >= 20:\n",
        "                    append_df(FIGHTERS_CSV, pd.DataFrame(fighters_batch))\n",
        "                    fighters_batch.clear()\n",
        "                if len(tot_overall_batch) >= 50:\n",
        "                    append_df(TOT_OVERALL_CSV, pd.DataFrame(tot_overall_batch))\n",
        "                    tot_overall_batch.clear()\n",
        "                if len(tot_round_batch) >= 50:\n",
        "                    append_df(TOT_ROUND_CSV, pd.DataFrame(tot_round_batch))\n",
        "                    tot_round_batch.clear()\n",
        "                if len(sig_overall_batch) >= 50:\n",
        "                    append_df(SIG_OVERALL_CSV, pd.DataFrame(sig_overall_batch))\n",
        "                    sig_overall_batch.clear()\n",
        "                if len(sig_round_batch) >= 50:\n",
        "                    append_df(SIG_ROUND_CSV, pd.DataFrame(sig_round_batch))\n",
        "                    sig_round_batch.clear()\n",
        "                if len(failures_batch) >= 10:\n",
        "                    append_df(FAIL_CSV, pd.DataFrame(failures_batch))\n",
        "                    failures_batch.clear()\n",
        "\n",
        "                time.sleep(0.1)\n",
        "            except Exception as fight_exc:\n",
        "                failures_batch.append(\n",
        "                    {\"url\": furl, \"type\": \"fight\", \"error\": str(fight_exc)}\n",
        "                )\n",
        "\n",
        "        append_df(FIGHTS_CSV, pd.DataFrame(fights_batch))\n",
        "        fights_batch.clear()\n",
        "        append_df(FIGHTERS_CSV, pd.DataFrame(fighters_batch))\n",
        "        fighters_batch.clear()\n",
        "        append_df(TOT_OVERALL_CSV, pd.DataFrame(tot_overall_batch))\n",
        "        tot_overall_batch.clear()\n",
        "        append_df(TOT_ROUND_CSV, pd.DataFrame(tot_round_batch))\n",
        "        tot_round_batch.clear()\n",
        "        append_df(SIG_OVERALL_CSV, pd.DataFrame(sig_overall_batch))\n",
        "        sig_overall_batch.clear()\n",
        "        append_df(SIG_ROUND_CSV, pd.DataFrame(sig_round_batch))\n",
        "        sig_round_batch.clear()\n",
        "        append_df(FAIL_CSV, pd.DataFrame(failures_batch))\n",
        "        failures_batch.clear()\n",
        "\n",
        "        state[\"event_idx\"] = ei + 1\n",
        "        save_state(state)\n",
        "        time.sleep(2.0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "id": "c0d9ec3c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 747 events. Resuming from event index 0.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Events:   0%|          | 0/747 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing event 0 of 747\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Events:   0%|          | 1/747 [00:19<3:56:33, 19.03s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing event 1 of 747\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Events:   0%|          | 1/747 [00:27<5:37:56, 27.18s/it]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[145]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Run the crawler (uncomment to execute; expect a long-running job)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mcrawl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[144]\u001b[39m\u001b[32m, line 284\u001b[39m, in \u001b[36mcrawl\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    281\u001b[39m         append_df(FAIL_CSV, pd.DataFrame(failures_batch))\n\u001b[32m    282\u001b[39m         failures_batch.clear()\n\u001b[32m--> \u001b[39m\u001b[32m284\u001b[39m     \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m fight_exc:\n\u001b[32m    286\u001b[39m     failures_batch.append(\n\u001b[32m    287\u001b[39m         {\u001b[33m\"\u001b[39m\u001b[33murl\u001b[39m\u001b[33m\"\u001b[39m: furl, \u001b[33m\"\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mfight\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33merror\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(fight_exc)}\n\u001b[32m    288\u001b[39m     )\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "# Run the crawler (uncomment to execute; expect a long-running job)\n",
        "crawl()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47a11260",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
